{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal Vectors and Subspaces\n",
    "A basis is a set of independent vectors that span a space. Geometrically, it is a set of coordinate axes (think about x- and y-axis in the x-y plane). In choosing a basis, we tend to choose an orthogonal basis. \n",
    "\n",
    "**orthonormal basis** is an orthogonal basis with length 1.\n",
    "\n",
    "**Length squared** of vector $x$: $||x||^{2}=x_{1}^{2}+x_{2}^{2}+\\cdots+x_{n}^{2}=x^{T}x$. The length squared is the inner product of x with itself.\n",
    ".\n",
    "**orthogonal test**: The *inner product* $x^Ty=0$ if and only if $x$ and $y$ are orthogonal vectors. If $x^Ty > 0$, their angle is less than 90°. If $x^Ty < 0$, their angle is greater than 90°. If x and y are orthogonal, length square of x plut length square of y equal with length square of (x + y). Unfold the right hand and use the fact that length square is $x^Tx$, we get $x^Ty=0$.\n",
    "\n",
    "If nonzero vectors $v_1,..., v_k$ are mutually orthogonal (every vector is perpendicular to every other), then those vectors are linearly independent.\n",
    "\n",
    "**Prthogonal subspaces**Two subspaces V and W of the same space $\\mathbb{R}^{n}$ are orthogonal if every vector v in V is orthogonal to every vector w in W: $v^Tw = 0$ for all v and w. By this definition, the wall and the floor are not orthogonal, they interscet with a line. Two plants cannot be orthogonal in 3-d space.\n",
    "\n",
    "**Fundamental theorem of orthogonality**: The row space is orthogonal to the nullspace (in $\\mathbb{R}^{n}$, each row of A times x equal 0). The column space is orthogonal to the left nullspace (in $\\mathbb{R}^{m}$).\n",
    "\n",
    "Given a subspace V of $\\mathbb{R}^{n}$, the space of all vectors orthogonal to V is called the **orthogonal complement** of V. It is denoted by $V^⊥$ = “V perp.”\n",
    "\n",
    "*The nullspace $N(A)$ is the **orthogonal complement** of the row space $C(A^T)$ in $\\mathbb{R}^{n}$*: the row space contains all vectors that are orthogonal to the nullspace.  \n",
    "*The left nullspace $N(A^T)$ is the **orthogonal complement** of the column space $C(A)$ in $\\mathbb{R}^{m}$*: The column space contains all vectors that are orthogonal to the left nullspace.   \n",
    "$Ax = b$ to be solvable, $b$ must to be in the column space, or indirectly, $b$ to be perpendicular to the left nullspace $N(A^T)$.\n",
    "\n",
    "![subspaces](http://i.imgur.com/AKYbfWa.png)\n",
    "\n",
    "Figure 3.4 summarizes the fundamental theorem of linear algebra. It illustrates the true effect of a matrix--what is happening inside the multiplication Ax. **Every matrix transforms its row space onto its column space**. The nullspace is carried to the zero vector. Every Ax is in the column space. Nothing is carried to the left nullspace. The real action is between the row space and column space, and you see it by looking at a typical vector x. It has a “row space component” and a “nullspace component,” with $x = x_r + x_n$. When multiplied by A, this is $Ax = Ax_r + Ax_n$:\n",
    "The nullspace component goes to zero: $Ax_n = 0$.\n",
    "The row space component goes to the column space: $Ax_r = Ax$.\n",
    "Of course everything goes to the column space—the matrix cannot do anything else.\n",
    "\n",
    "Every vector b in the column space comes from **exactly one** vector $x_r$ in the row space. On those r-dimensional spaces A is invertible.\n",
    "\n",
    "$A^T$ goes in the opposite direction, from $\\mathbb{R}^{m}$ to $\\mathbb{R}^{n}$ and from $C(A)$ back to $C(A^T)$. Of course the transpose is not the inverse! $A^T$ moves the spaces correctly, but not the individual vectors. That honor belongs to $A^{−1}$ if it exists--and it only exists if $r= m= n$ (i.e, square matrix with full rank). We cannot ask $A^{−1}$ to bring back a whole nullspace out of the zero vector.\n",
    "\n",
    "When $A^{−1}$ fails to exist, the best substitute is the pseudoinverse $A^+$. This inverts A where that is possible: $A^+Ax = x$ for x in the row space. On the left nullspace, nothing can be done: $A^+y = 0$. Thus $A^+$ inverts A where it is invertible, and has the same rank r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![Imgur](http://i.imgur.com/MEhViPQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of line, b can also project to any subspace. \n",
    "\n",
    "### why projection?\n",
    "When $A^{−1}$ fails to exist, i.e. $Ax = b$ has no solution, the best substitute is the pseudoinverse $A^+$ -- solve the closest problem we can solve. $Ax$ will always be in the column space of A, but b is not necessary to be in the column space (thus no solution). So we can choose the closest vector (compare with b) in the column space. So we will solve $A\\hat{x} = P$ instead, where $P$ is the projection of $b$ onto the column space. $\\hat{x}$ is the best possible solution. \n",
    "\n",
    "In linear regression $y ~ x$, this is saying that it is impossible to link every points with one line, instead we will try to find a line with least square of errors.\n",
    "\n",
    "The **cosine** of the angle between any nonzero vectors a and b is $\\cos\\theta=\\frac{a^{T}b}{||a||||b||}$. Because $|\\cos\\theta| \\leq 1$, this gives the **Schwarz inequality**: $|a^Tb| \\leq ||a||||b||$\n",
    "\n",
    "Law of cosines: $||b-a||^{2}=||b||^{2}+||a||^{2}-2||b||||a||\\cos\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection onto a line\n",
    "p must be = $\\hat{x}a$ since they are on the same line. To get p, all we need is to computer $\\hat{x}$. We know $a\\perp(b-\\hat{x}a)$ --> $a^T(b-\\hat{x}a)=0$ --> $\\hat{x}a^Ta = a^Tb$ --> $\\hat{x}=\\frac{a^Tb}{a^Ta}$. Thus, **projection onto a line** $p = \\hat{x}a = \\frac{a^Tb}{a^Ta}a$.\n",
    "\n",
    "**Projection matrix *P:*** ($p = Pb$) is the matrix that multiples $b$ and produces $p$: $p=\\frac{a^Tb}{a^Ta}a = a\\frac{a^Tb}{a^Ta} = \\frac{aa^T}{a^Ta}b$, so $P = \\frac{aa^T}{a^Ta}$. \n",
    "\n",
    "- $aa^T$ is a square matrix, $a^Ta$ is a number. So *P* is a square matrix.\n",
    "- *P* is symmetric.\n",
    "- $P^2 = P$: $P^2b$ is the projection of $Pb$ and $Pb$ is already on the line.\n",
    "- Column space of *P* consists of the line through a.\n",
    "- the rank is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
